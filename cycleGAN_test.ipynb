{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from architectures.cycleGAN_u_net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='TRAIN'\n",
    "#task='TEST'\n",
    "SEED=1\n",
    "\n",
    "PATH = 'cycleGAN_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 5000\n",
    "validating_size=1000\n",
    "\n",
    "LEARNING_RATE = 2e-4\n",
    "BETA1 = 0.5\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "SAVE_SAMPLE_PERIOD = 50\n",
    "SEED = 1\n",
    "preprocess=False\n",
    "cost_type='GAN'\n",
    "\n",
    "cycl_weight=10.\n",
    "discr_steps=1\n",
    "gen_steps=4\n",
    "\n",
    "ndf = 4\n",
    "ngf = 8\n",
    "s=2\n",
    "f = 2\n",
    "d=0.8\n",
    "\n",
    "stddev_d=0.02\n",
    "stddev_g=0.02\n",
    "\n",
    "global d_sizes, g_sizes\n",
    "\n",
    "g_sizes_enc={\n",
    "    \n",
    "        'conv_layers': [\n",
    "                            (ngf/2, f*16, s,  False, 1, lrelu,  tf.glorot_normal_initializer()), #(batch, 52, 64, 1) =>  (batch, 26, 32, ngf)\n",
    "                            (ngf,   f*8, s,  False,  1, lrelu,  tf.glorot_normal_initializer()), #(batch, 52, 64, 1) =>  (batch, 26, 32, ngf)\n",
    "                            (ngf*2, f*6, s, 'bn',    d, lrelu,  tf.glorot_normal_initializer()),#(batch, 26, 32, ngf) => (batch, 13, 16, ngf*2)\n",
    "                            (ngf*4, f*4, s, 'bn',    d, lrelu,  tf.glorot_normal_initializer()),#(batch, 13, 16, ngf*4) => (batch, 7, 8, ngf*4)\n",
    "                            (ngf*8, f*2, s, 'bn',    d, lrelu,  tf.glorot_normal_initializer()),#(batch, 7, 8, ngf*4) => (batch, 4, 4, ngf*4)\n",
    "                            (ngf*8, f,   1, 'bn',    d, lrelu,  tf.glorot_normal_initializer()),#(batch, 4, 4, ngf*4) => (batch, 2, 2, ngf*4)\n",
    "                           #(ngf*8, f, s, 'bn', 1, lrelu, tf.truncated_normal_initializer(stddev_g)),#(batch, 2, 2, ngf*4) => (batch, 1, 1, ngf*4)\n",
    "                        \n",
    "                       ],\n",
    "}\n",
    "\n",
    "g_sizes_dec={\n",
    "\n",
    "     \n",
    "     'deconv_layers': [\n",
    "\n",
    "                        (ngf*8, f,   1,   'bn', d, tf.nn.relu,    tf.glorot_normal_initializer()),#(batch, 1, 1, ngf*4) => (batch, 2, 2, ngf*4*2)\n",
    "                        (ngf*4, f*2, s,   'bn', d, tf.nn.relu,    tf.glorot_normal_initializer()),#(batch, 2, 2, ngf*4*2) => (batch, 4, 4, ngf*4*2)\n",
    "                        (ngf*2, f*4, s,   'bn', d, tf.nn.relu,    tf.glorot_normal_initializer()),#(batch, 4, 4, ngf*4*2) => (batch, 7, 8, ngf*4*2)\n",
    "                        (ngf,   f*6, s,   'bn', d, tf.nn.relu,    tf.glorot_normal_initializer()),#(batch, 7, 8, ngf*4*2) => (batch, 13, 16, ngf*2*2)\n",
    "                        (ngf/2, f*16,s,  'bn', 1,  tf.nn.relu,    tf.glorot_normal_initializer()), #(batch, 52, 64, 1) =>  (batch, 26, 32, ngf)\n",
    "                        (3,     f*16,s, False, 1,  tf.sigmoid,    tf.glorot_normal_initializer()),#(batch, 26, 32, ngf*2) => (batch, 52, 64, 1)\n",
    "                   \n",
    "                     ],  \n",
    "}\n",
    "\n",
    "\n",
    "d_sizes={\n",
    "    \n",
    "     'conv_layers': [\n",
    "                         (ndf/2, f*4, s*2,  False, 1, lrelu, tf.truncated_normal_initializer(stddev_d)), #(batch, 52, 64, 2) => (batch, 26, 32, ndf)\n",
    "                         (ndf,   f*2, s,   'bn',   d, lrelu, tf.truncated_normal_initializer(stddev_d)), #(batch, 26, 32, ndf) => (batch, 13, 16, ndf*2)\n",
    "                         (ndf*2, f,   s,   'bn',   d, lrelu, tf.truncated_normal_initializer(stddev_d)), #(batch, 13, 16, ndf*2) => (batch, 7, 8, ndf*4)\n",
    "                         (ndf*4, f,   s,   'bn',   1, lrelu, tf.truncated_normal_initializer(stddev_d)), #(batch, 7, 8, ndf*4) => (batch, 7, 8, ndf*8)\n",
    "                        #(ndf*8, f, 1, 'bn', 1, lrelu, tf.truncated_normal_initializer(stddev_d)), #(batch, 7, 8, ndf*8) => (batch, 7, 8, ndf*8)\n",
    "                         \n",
    "                    ],\n",
    "                    \n",
    "     #'readout_conv_layer':[\n",
    "     #                    (f,   s,  False,   d,  tf.truncated_normal_initializer(stddev_d) )\n",
    "     #],\n",
    "     'dense_layers': [(ndf*32, 'bn',  d, lrelu, tf.truncated_normal_initializer(stddev_d)),\n",
    "                     (ndf,     False, d, lrelu, tf.truncated_normal_initializer(stddev_d))],\n",
    "     \n",
    "     'readout_layer_w_init':tf.truncated_normal_initializer(stddev_d),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Horse2Zebra():\n",
    "    \n",
    "    train_A = np.array(\n",
    "        [plt.imread(\"apple2orange/trainA/\"+filename) for filename in os.listdir(\"apple2orange/trainA\")]\n",
    "    )\n",
    "    \n",
    "    train_B = np.array(\n",
    "        [plt.imread(\"apple2orange/trainB/\"+filename) for filename in os.listdir(\"apple2orange/trainB\")]\n",
    "    )\n",
    "    \n",
    "    m = np.minimum(train_A.shape[0],train_B.shape[0])\n",
    "    \n",
    "    _, n_H, n_W, n_C = train_A.shape\n",
    "    \n",
    "    X_train_A = train_A[0:m]/256\n",
    "    X_train_B = train_B[0:m]/256\n",
    "    \n",
    "    #train_A=train_A.astype(np.float32)\n",
    "    #mean_A=np.mean(train_A,axis=0)\n",
    "    #train_A_1=train_A-mean_A\n",
    "    #std_A=np.std(train_A_1,axis=0)\n",
    "    #train_A_2=train_A_1/std_A\n",
    "    #\n",
    "    #train_A=train_A_2\n",
    "    #\n",
    "    #train_B=train_B.astype(np.float32)\n",
    "    #mean_B=np.mean(train_B,axis=0)\n",
    "    #train_B_1=train_B-mean_B\n",
    "    #std_B=np.std(train_B_1,axis=0)\n",
    "    #train_B_2=train_B_1/std_B\n",
    "    #\n",
    "    #train_B = train_B_2\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    gan = cycleGAN_u_net(n_H_A=n_H, n_W_A=n_W,\n",
    "                         n_H_B=n_H, n_W_B=n_W, n_C=n_C, \n",
    "                         mean_A=mean_A, std_A=std_A, \n",
    "                         mean_B=mean_B, std_B=std_B, \n",
    "                         d_sizes_A=d_sizes, d_sizes_B=d_sizes,\n",
    "                         g_sizes_enc_A=g_sizes_enc, g_sizes_enc_B= g_sizes_enc,\n",
    "                         g_sizes_dec_A=g_sizes_dec, g_sizes_dec_B= g_sizes_dec,\n",
    "                         lr=LEARNING_RATE,beta1=BETA1, preprocess=False,\n",
    "                         cost_type='GAN', gan_weight=1, cycl_weight=cycl_weight,\n",
    "                         discr_steps=discr_steps, gen_steps=gen_steps,\n",
    "                         batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                         save_sample=SAVE_SAMPLE_PERIOD, path=PATH, seed=SEED)\n",
    "    \n",
    "    vars_to_train= tf.trainable_variables()\n",
    "    \n",
    "    if task == 'TRAIN':\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    if task == 'TEST':\n",
    "        vars_all = tf.global_variables()\n",
    "        vars_to_init = list(set(vars_all)-set(vars_to_train))\n",
    "        init_op = tf.variables_initializer(vars_to_init)\n",
    "        \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()    \n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "\n",
    "        if task=='TRAIN':\n",
    "            print('\\n Training...')\n",
    "            \n",
    "            if os.path.exists(PATH+'/'+PATH+'.ckpt.index'):\n",
    "                saver.restore(sess,PATH+'/'+PATH+'.ckpt')\n",
    "                print('Model restored.')\n",
    "            \n",
    "            gan.set_session(sess)\n",
    "            unact_A, unact_B = gan.fit(X_train_A, X_train_B, 1)\n",
    "            \n",
    "            save_path = saver.save(sess, PATH+'/'+PATH+'.ckpt')\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "        if task=='TEST':\n",
    "            print('\\n Evaluate model on test set...')\n",
    "            saver.restore(sess,PATH+'/'+PATH+'.ckpt')\n",
    "            print('Model restored.')\n",
    "            \n",
    "            gan.set_session(sess) \n",
    "            \n",
    "        done = True\n",
    "        while not done:\n",
    "            \n",
    "            \n",
    "            j = np.random.choice(len(X_train_A))\n",
    "            true_img = X_train_A[j]\n",
    "            sample_img = gan.get_sample(true_img)\n",
    "            \n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(true_img.reshape(n_H,n_W),cmap='gray')\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(sample_img.reshape(n_H,n_W),cmap='gray')\n",
    "            \n",
    "            fig=plt.gcf()\n",
    "            fig.set_size_inches(5,8)\n",
    "            plt.savefig(PATH+'/sample_{0}_at_iter_{1}.png'.format(j, total_iters),dpi=300)\n",
    "            \n",
    "            ans = input(\"Generate another?\")\n",
    "            if ans and ans[0] in ('n' or 'N'):\n",
    "                done = True\n",
    "                \n",
    "    return unact_A, unact_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Network architecture detected for discriminator A\n",
      "Convolutional Network architecture detected for discriminator B\n",
      "Generator_A_to_B\n",
      "Input for generator encoder shape (?, 256, 256, 3)\n",
      "Output of generator encoder, \n",
      " and input for generator decoder shape (?, 1, 1, 4096)\n",
      "Generator output shape (?, 256, 256, 3)\n",
      "Generator_B_to_A\n",
      "Input for generator encoder shape (?, 256, 256, 3)\n",
      "Output of generator encoder, \n",
      " and input for generator decoder shape (?, 1, 1, 4096)\n",
      "Generator output shape (?, 256, 256, 3)\n",
      "Generator_B_to_A\n",
      "Input for generator encoder shape (?, 256, 256, 3)\n",
      "Output of generator encoder, \n",
      " and input for generator decoder shape (?, 1, 1, 4096)\n",
      "Generator output shape (?, 256, 256, 3)\n",
      "Generator_B_to_A\n",
      "Input for generator encoder shape (?, 256, 256, 3)\n",
      "Output of generator encoder, \n",
      " and input for generator decoder shape (?, 1, 1, 4096)\n",
      "Generator output shape (?, 256, 256, 3)\n",
      "Generator_A_to_B\n",
      "Input for generator encoder shape (?, 256, 256, 3)\n",
      "Output of generator encoder, \n",
      " and input for generator decoder shape (?, 1, 1, 4096)\n",
      "Generator output shape (?, 256, 256, 3)\n",
      "Generator_A_to_B\n",
      "Input for generator encoder shape (?, 256, 256, 3)\n",
      "Output of generator encoder, \n",
      " and input for generator decoder shape (?, 1, 1, 4096)\n",
      "Generator output shape (?, 256, 256, 3)\n",
      "Discriminator_B\n",
      "Convolution\n",
      "Input for convolution shape  (?, 256, 256, 3)\n",
      "Logits shape (?, 1)\n",
      "Discriminator_B\n",
      "Convolution\n",
      "Input for convolution shape  (?, 256, 256, 3)\n",
      "Logits shape (?, 1)\n",
      "Discriminator_A\n",
      "Convolution\n",
      "Input for convolution shape  (?, 256, 256, 3)\n",
      "Logits shape (?, 1)\n",
      "Discriminator_A\n",
      "Convolution\n",
      "Input for convolution shape  (?, 256, 256, 3)\n",
      "Logits shape (?, 1)\n",
      "\n",
      " Training...\n",
      "\n",
      " ****** \n",
      "\n",
      "Training cycle GAN with pix2pix gen/disc with a total of 994 samples distributed in 994 batches of size 1\n",
      "\n",
      "The validation set consists of 1 images\n",
      "The learning rate is 0.0002, and every 50 batches a generated sample will be saved to cycleGAN_test\n",
      "\n",
      " ****** \n",
      "\n",
      "Epoch: 0\n",
      "At iter: 50  -  dt: 0:00:00.815527 \n",
      "At iter: 50  -  dt: 0:00:00.816631 \n",
      "Discrimator_A cost 2.748, Generator_B_to_A cost 11.33\n",
      "Discrimator_B cost 2.796, Generator_A_to_B cost 11.27\n",
      "Saving a sample...\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    if task == 'TRAIN':\n",
    "        if not os.path.exists(PATH):\n",
    "            os.mkdir(PATH)\n",
    "    \n",
    "        elif os.path.exists(PATH):\n",
    "            if os.path.exists(PATH+'/checkpoint'):\n",
    "                ans = input('A previous checkpoint already exists, choose the action to perform \\n \\n 1) Overwrite the current model saved at '+PATH+'/checkpoint \\n 2) Start training a new model \\n 3) Restore and continue training the previous model \\n ')\n",
    "                \n",
    "                if ans == '1':\n",
    "                    print('Overwriting existing model in '+PATH)\n",
    "                    for file in os.listdir(PATH):\n",
    "                        file_path = os.path.join(PATH, file)\n",
    "                        try:\n",
    "                            if os.path.isfile(file_path):\n",
    "                                os.unlink(file_path)\n",
    "                            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            \n",
    "                elif ans == '2':\n",
    "                    PATH = input('Specify the name of the model, a new directory will be created.\\n')\n",
    "                    os.mkdir(PATH)    \n",
    "        \n",
    "        unact_A, unact_B = Horse2Zebra()\n",
    "   \n",
    "    elif task == 'TEST': \n",
    "        if not os.path.exists(PATH+'/checkpoint'):\n",
    "            print('No checkpoint to test')\n",
    "        else:\n",
    "            Horse2Zebra()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(unact_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
